{"cells":[{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.types import *  # Necessary for creating schemas\nfrom pyspark.sql.functions import * # Importing PySpark functions"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Make a tuple list\na_list = [('a', 1), ('b', 2), ('c', 3)]\n\n# Create a Spark DataFrame, without supplying a schema value\ndf_from_list_no_schema = \\\nsqlContext.createDataFrame(a_list)\n\n# Print the DF object\nprint df_from_list_no_schema\n\n# Print a collected list of Row objects\nprint df_from_list_no_schema.collect()\n\n# Show the DataFrame\ndf_from_list_no_schema.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Create a Spark DataFrame, this time with schema\ndf_from_list_with_schema = \\\nsqlContext.createDataFrame(a_list, ['letters', 'numbers'])\n\n# Show the DataFrame\ndf_from_list_with_schema.show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Make a dictionary\na_dict = [{'letters': 'a', 'numbers': 1},\n          {'letters': 'b', 'numbers': 2},\n          {'letters': 'c', 'numbers': 3}]\n\n# Create a Spark DataFrame from the dictionary\ndf_from_dict = \\\nsqlContext.createDataFrame(a_dict)\n\n# Show the DataFrame\ndf_from_dict.show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Define the schema\nschema = StructType([\n    StructField('letters', StringType(), True),\n    StructField('numbers', IntegerType(), True)])\n\n# Create an RDD from a list\nrdd = sc.parallelize(a_list)\n\n# Create the DataFrame from these raw components\nnice_df = \\\n(sqlContext\n .createDataFrame(rdd, schema))\n\n# Show the DataFrame\nnice_df.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# We now have a `nice_df`, here are some nice functions for\n# inspecting the DF\n\n# `columns`: return all column names as a list\nnice_df.columns"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# `dtypes`: get the datatypes for all columns\nnice_df.dtypes"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# `printSchema()`: prints the schema of the supplied DF\nnice_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# `schema`: returns the schema of the provided DF as `StructType` schema\nnice_df.schema"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# `first()` returns the first row as a Row while\n# `head()` and `take()` return `n` number of Row objects\nprint nice_df.first() # can't supply a value; never a list\nprint nice_df.head(2) # can optionally supply a value (default: 1);\n                      # with n > 1, a list\nprint nice_df.take(2) # expects a value; always a list"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# `count()`: returns a count of all rows in DF\nnice_df.count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# `describe()`: print out stats for numerical columns\nnice_df.describe().show() # can optionally supply a list of column names"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# the `explain()` function explains the under-the-hood evaluation process\nnice_df.explain()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# `unionAll()`: combine two DataFrames together\n\n# Take the DataFrame and add it to itself\ndouble_df = \\\n(nice_df\n .unionAll(nice_df))\n\n# Add it to itself twice\ntriple_df = \\\n(nice_df\n .unionAll(nice_df)\n .unionAll(nice_df))\n\n# Coercion will occur if schemas don't align\ncoerced_to_strings_df = \\\n(nice_df\n .select(['numbers', 'letters'])\n .unionAll(nice_df))\n\n#coerced_to_strings_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(double_df)\n#display(triple_df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# `orderBy()` will perform sorting of DataFrame columns\nsorted_asc_triple_df = \\\n(triple_df\n .orderBy('numbers'))\n\nsorted_desc_triple_df = \\\n(triple_df\n .orderBy('numbers', ascending = False))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display(sorted_asc_triple_df)\n#display(sorted_desc_triple_df)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# `select()` and `drop()` both take a list of column names\n# and these functions do exactly what you might expect\n\n# Select only the first column of the DF\nnice_df_1_col = \\\n(nice_df\n .select('letters'))\n\n# Re-order columns in the DF using `select()`\nnice_df_2_col_reordered = \\\n(nice_df\n .select(['numbers', 'letters']))\n\n# Drop the second column of the DF\nnice_df_1_col_drop = \\\n(nice_df\n .drop('letters'))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# The `filter()` function performs filtering of DF rows\n\n# Here is some numeric filtering with comparison operators\n# (>, <, >=, <=, ==, != all work)\nnice_df.filter(nice_df.numbers > 1).show()\nnice_df.filter(nice_df.numbers > 1).filter(nice_df.numbers < 3).show()\n\n# Not just numbers! Use the `filter()` + `isin()` combo to\n# filter on string columns with a set of values\nnice_df.filter(nice_df.letters.isin(['a', 'b'])).show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Before we get into aggregations, let's\n# load in a CSV with interesting data and\n# create a new DataFrame\n\n# You do this with the `spark-csv` package;\n# documentation: https://github.com/databricks/spark-csv\n\n# This dataset contains data about flights departing\n# New York City airports (JFK, LGA, EWR) in 2013; it\n# has 336,776 rows and 16 columns\n\n# Create a schema object then read the CSV with schema\nnycflights_schema = StructType([\n  StructField('year', IntegerType(), True),\n  StructField('month', IntegerType(), True),\n  StructField('day', IntegerType(), True),\n  StructField('dep_time', StringType(), True),\n  StructField('dep_delay', IntegerType(), True),\n  StructField('arr_time', StringType(), True),\n  StructField('arr_delay', IntegerType(), True),\n  StructField('carrier', StringType(), True),\n  StructField('tailnum', StringType(), True),\n  StructField('flight', StringType(), True),  \n  StructField('origin', StringType(), True),\n  StructField('dest', StringType(), True),\n  StructField('air_time', IntegerType(), True),\n  StructField('distance', IntegerType(), True),\n  StructField('hour', IntegerType(), True),\n  StructField('minute', IntegerType(), True)\n  ])\n\nnycflights = \\\n(sqlContext\n .read\n .format('com.databricks.spark.csv')\n .schema(nycflights_schema)\n .options(header = True)\n .load('/mnt/spark-atp/nycflights13/nycflights13.csv'))\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Have a look at the schema for the imported dataset\nnycflights.printSchema()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Have a look at the `nycflights` DataFrame with the `display()` function (available for Databricks Cloud notebooks)\ndisplay(nycflights)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Let's group and aggregate\n\n# `groupBy()` will group one or more DF columns\n# and prep them for aggregration functions\n(nycflights\n .groupby('month') # creates 'GroupedData'\n .count() # creates a new column with aggregate `count` values\n .show())\n\n# Use the `agg()` function to perform multiple\n# aggregations\n(nycflights\n .groupby('month')\n .agg({'dep_delay': 'avg', 'arr_delay': 'avg'}) # note the new column names\n .show())\n\n# Caveat: you can't perform multiple aggregrations\n# on the same column (only the last is performed)\n(nycflights\n .groupby('month')\n .agg({'dep_delay': 'min', 'dep_delay': 'max'})\n .show())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Use `groupBy()` with a few columns, then aggregate\ndisplay(\n  nycflights\n  .groupby(['month', 'origin', 'dest']) # group by these unique combinations\n  .count()                              # perform a 'count' aggregation on the groups\n  .orderBy(['month', 'count'],\n           ascending = [1, 0])          # order by `month` ascending, `count` descending\n) "],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Use `groupBy()` + `pivot()` + an aggregation function to\n# make a pivot table!\n\n# Get a table of flights by month for each carrier\ndisplay(\n  nycflights\n  .groupBy('month') # group the data for aggregation by `month` number\n  .pivot('carrier') # provide columns of data by `carrier` abbreviation\n  .count()          # create aggregations as a count of rows\n)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Another pivot table idea: get the average departure\n# delay for each carrier at the different NYC airports\ndisplay(\n  nycflights\n  .groupBy('carrier')\n  .pivot('origin')\n  .avg('dep_delay')\n)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Perform 2 different aggregations, rename those new columns,\n# then do some rounding of the aggregrate values\ndisplay(\n  nycflights\n  .groupby('month')\n  .agg({'dep_delay': 'avg', 'arr_delay': 'avg'})\n  .withColumnRenamed('avg(arr_delay)', 'mean_arr_delay')\n  .withColumnRenamed('avg(dep_delay)', 'mean_dep_delay')\n  .withColumn('mean_arr_delay', format_number('mean_arr_delay', 1))\n  .withColumn('mean_dep_delay', format_number('mean_dep_delay', 1))\n)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Add a new column (`far_or_near`) with a string based on a comparison\n# on a numeric column; uses: `withColumn()`, `when()`, and `otherwise()`\ndisplay(\n  nycflights\n  .withColumn('far_or_near',\n              when(nycflights.distance > 1000, 'far') # the `if-then` statement\n              .otherwise('near'))                     # the `else` statement\n)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Perform a few numerical computations across columns\ndisplay(\n  nycflights\n  .withColumn('dist_per_minute',\n              nycflights.distance / nycflights.air_time) # create new column with division of values\n  .withColumn('dist_per_minute',\n              format_number('dist_per_minute', 2))       # round that new column's float value to 2 decimal places\n  .drop('distance') # drop the `distance` column\n  .drop('air_time') # drop the `air_time` column\n)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Create a proper timestamp for once in your life...\n# We have all the components: `year`, `month`, `day`,\n# `hour`, and `minute`\n\n# Use `concat_ws()` (concatentate with separator) to\n# combine column data into StringType columns such\n# that dates (`-` separator, YYYY-MM-DD) and times\n# (`:` separator, 24-hour time) are formed\nnycflights = \\\n(nycflights\n .withColumn('date',\n             concat_ws('-',\n                       nycflights.year,\n                       nycflights.month,\n                       nycflights.day))\n .withColumn('time',\n             concat_ws(':',\n                       nycflights.hour,\n                       nycflights.minute)))\n\n# In a second step, concatenate with `concat_ws()`\n# the `date` and `time` strings (separator is a space);\n# then drop several columns\nnycflights = \\\n(nycflights\n .withColumn('timestamp',\n             concat_ws(' ',\n                       nycflights.date,\n                       nycflights.time))\n .drop('year')     # `drop()` doesn't accept\n .drop('month')    # a list of column names,\n .drop('day')      # therefore, for every column\n .drop('hour')     # we would like to remove\n .drop('minute')   # from the DataFrame, we \n .drop('date')     # must create a new `drop()`\n .drop('time'))    # statement\n\n# In the final step, convert the `timestamp` from\n# a StringType into a TimestampType\nnycflights = \\\n(nycflights\n .withColumn('timestamp',\n             to_utc_timestamp(nycflights.timestamp, 'GMT')))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Display the `nycflights` DataFrame with the new `timestamp` column\ndisplay(nycflights)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# It probably doesn't matter in the end, but,\n# I'd prefer that the `timestamp` column be\n# the first column; let's make use of the\n# `columns` method and get slicing!\nnycflights = \\\n (nycflights\n  .select(nycflights.columns[-1:] + nycflights.columns[0:-1])) # recall that `columns` returns a list of column names"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Now the `timestamp` column is the first column: \ndisplay(nycflights)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Inspect the DataFrame's schema, note that `timestamp` is indeed classed as a timestamp\nnycflights.printSchema()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# If you miss the time component columns,\n# you can get them back! Use the `year()`,\n# `month()`, `dayofmonth()`, `hour()`, and\n# `minute()` functions with `withColumn()`\ndisplay(\n  nycflights\n  .withColumn('year', year(nycflights.timestamp))\n  .withColumn('month', month(nycflights.timestamp))\n  .withColumn('day', dayofmonth(nycflights.timestamp))\n  .withColumn('hour', hour(nycflights.timestamp))\n  .withColumn('minute', minute(nycflights.timestamp))\n)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# There are more time-based functions:\n# `date_sub()` - subtract an integer number of days from a\n#                Date or Timestamp\n# `date_add()` - add an integer number of days from a\n#                Date or Timestamp\n# `datediff()` - get the difference between two dates\n# `add_months()` - add an integer number of months\n# `months_between()` - get the number of months between\n#                      two dates\n# `next_day()` - returns the first date which is later\n#                than the value of the date column\n# `last_day()` - returns the last day of the month which\n#                the given date belongs to\n# `dayofmonth()` - extract the day of the month of a\n#                  given date as integer\n# `dayofyear()` - extract the day of the year of a given\n#                 date as integer\n# `weekofyear()` - extract the week number of a given\n#                  date as integer\n# `quarter()` - extract the quarter of a given date\n\n# Let's transform the timestamp in the first\n# record of `nycflights` with each of these\n# functions\ndisplay(\n  nycflights\n   .limit(1)\n   .select('timestamp')\n   .withColumn('date_sub', date_sub(nycflights.timestamp, 7))\n   .withColumn('date_add', date_add(nycflights.timestamp, 7))\n   .withColumn('datediff', datediff(nycflights.timestamp, nycflights.timestamp))\n   .withColumn('add_months', add_months(nycflights.timestamp, 1))\n   .withColumn('months_between', months_between(nycflights.timestamp, nycflights.timestamp))\n   .withColumn('next_day', next_day(nycflights.timestamp, 'Mon'))\n   .withColumn('last_day', last_day(nycflights.timestamp))\n   .withColumn('dayofmonth', dayofmonth(nycflights.timestamp))\n   .withColumn('dayofyear', dayofyear(nycflights.timestamp))\n   .withColumn('weekofyear', weekofyear(nycflights.timestamp))\n   .withColumn('quarter', quarter(nycflights.timestamp))\n   )"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Joins are easily performed with Spark DataFrames. The expression is:\n\n`join(other, on = None, how = None)`\n\nwhere:\n- other: a DataFrame that serves as the right side of the join\n- on: typically a join expression\n- how: the default is `inner` but there are also `inner`, `outer`, `left_outer`, `right_outer`, and `leftsemi` joins available"],"metadata":{}},{"cell_type":"code","source":["# Let's load in some more data so that we can have two DataFrames to join\n\n# The CSV `weather.csv` contains hourly meteorological data from EWR during 2013\n\n# Create a schema object then read the CSV with schema\nweather_schema = StructType([  \n  StructField('year', IntegerType(), True),\n  StructField('month', IntegerType(), True),\n  StructField('day', IntegerType(), True),\n  StructField('hour', IntegerType(), True),\n  StructField('temp', FloatType(), True),\n  StructField('dewp', FloatType(), True),\n  StructField('humid', FloatType(), True),\n  StructField('wind_dir', IntegerType(), True),\n  StructField('wind_speed', FloatType(), True),\n  StructField('wind_gust', FloatType(), True),\n  StructField('precip', FloatType(), True),\n  StructField('pressure', FloatType(), True),\n  StructField('visib', FloatType(), True)\n  ])\n\nweather = \\\n(sqlContext\n .read\n .format('com.databricks.spark.csv')\n .schema(weather_schema)\n .options(header = True)\n .load('/mnt/spark-atp/nycflights13/weather.csv'))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# Have a look at the imported dataset\ndisplay(weather)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# We need those `month`, `day`, and `hour` values back\nnycflights = \\\n(nycflights\n .withColumn('month', month(nycflights.timestamp))\n .withColumn('day', dayofmonth(nycflights.timestamp))\n .withColumn('hour', hour(nycflights.timestamp)))\n\n# Join the `nycflights` DF with the `weather` DF \nnycflights_all_columns = \\\n(nycflights\n .join(weather,\n       [nycflights.month == weather.month, # three join conditions: month,\n        nycflights.day == weather.day,     #                        day,\n        nycflights.hour == weather.hour],  #                        hour\n       'left_outer')) # left outer join: keep all rows from the left DF (flights), with the matching rows in the right DF (weather)\n                      # NULLs created if there is no match to the right DF"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Notice that lots of columns created, as well as duplicate column names (not a bug! a feature?)\ndisplay(nycflights_all_columns)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# One way to reduce the number of extraneous\n# columns is to use a `select()` statement\nnycflights_wind_visib = \\\n(nycflights_all_columns\n .select(['timestamp', 'carrier', 'flight',\n          'origin', 'dest', 'wind_dir',\n          'wind_speed', 'wind_gust', 'visib']))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# Examine the DataFrame, now with less columns\ndisplay(nycflights_wind_visib)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["# Let's load in even more data so we can determine if\n# any takeoffs occurred in very windy weather\n\n# The CSV `beaufort_land.csv` contains Beaufort scale values\n# and wind speed ranges in mph\n\n# Create a schema object then read the CSV with schema\nbeaufort_land_schema = StructType([  \n  StructField('force', IntegerType(), True),\n  StructField('speed_mi_h_lb', IntegerType(), True),\n  StructField('speed_mi_h_ub', IntegerType(), True),\n  StructField('name', StringType(), True)\n  ])\n\nbeaufort_land = \\\n(sqlContext\n .read\n .format('com.databricks.spark.csv')\n .schema(beaufort_land_schema)\n .options(header = True)\n .load('/mnt/spark-atp/nycflights13/beaufort_land.csv'))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# Have a look at the imported dataset\ndisplay(beaufort_land)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# Join the current working DF with the `beaufort_land` DF\n# and use join expressions that use the WS ranges\nnycflights_wind_visib_beaufort = \\\n(nycflights_wind_visib\n .join(beaufort_land,\n      [nycflights_wind_visib.wind_speed >= beaufort_land.speed_mi_h_lb,\n       nycflights_wind_visib.wind_speed < beaufort_land.speed_mi_h_ub],\n       'left_outer')\n .withColumn('month', month(nycflights_wind_visib.timestamp)) # Create a month column from `timestamp` values\n)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# View the joined DF; now we have extra data on wind speed!\ndisplay(nycflights_wind_visib_beaufort)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# We can inspect the number of potentially dangerous\n# takeoffs (i.e., where the Beaufort force is high)\n# month-by-month through the use of the `crosstab()` function\ncrosstab_month_force = \\\n(nycflights_wind_visib_beaufort\n .crosstab('month', 'force'))\n\ncrosstab_month_force = \\\n(crosstab_month_force\n .withColumn('month_force',\n             crosstab_month_force.month_force.cast('int')) # the column is initially a string but recasting as\n                                                           # an `int` will aid ordering in the next expression\n .orderBy('month_force')\n .drop('null'))"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# Display the cross tabulation; turns out January was a bit riskier for takeoffs due to wind conditions\ndisplay(crosstab_month_force)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# Saving to CSV is quite similar to reading from a CSV file\n(crosstab_month_force\n .write\n .mode('overwrite')\n .format('com.databricks.spark.csv')\n .save('/mnt/spark-atp/nycflights13/crosstab_month_force/'))"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# Saving to Parquet is generally recommended for data warehousing purposes\n(crosstab_month_force\n .write\n .mode('overwrite')\n .parquet('/mnt/spark-atp/nycflights13/crosstab_month_force_parquet/'))"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["There are many more functions... although I tried to cover a lot of ground, there are dozens more functions for DataFrames that I haven't touched upon.\n\nThe main reference for PySpark is:\n\n- http://spark.apache.org/docs/latest/api/python/index.html\n\nThese examples are available at:\n\n- https://github.com/rich-iannone/so-many-pyspark-examples"],"metadata":{}}],"metadata":{"name":"spark-dataframes","notebookId":96485},"nbformat":4,"nbformat_minor":0}
